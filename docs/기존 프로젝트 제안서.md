# KISA TTPs 보고서 기반 Caldera 공격 시나리오 자동 생성 및 검증 시스템

→ Self-Correcting 기반 공격 시나리오 자동 생성 및 실행 검증 시스템

## 1. 추진 배경

### 1-1. 현실의 문제: 급증하는 사이버 공격과 대응의 어려움

2024년 한국의 사이버 침해사고 신고 건수는 1,887건으로, 전년도 1,277건 대비 48%나 급증했다. 단순한 숫자가 아니다. 각 건수 뒤에는 실제 피해를 입은 기업과 기관이 있다.

2024년 12월, 국내 대형 법무법인이 북한 연계 해킹 그룹 Kimsuky의 표적이 되었다. 공격자는 '한국방위산업학회 세미나' 문서로 위장한 악성 파일을 발송했고, 이 파일을 열면 시스템에 은밀히 침투하여 15분마다 민감한 정보를 외부로 전송했다. 같은 시기, 북한 정부 지원 해킹 조직 Lazarus는 '오퍼레이션 싱크홀'이라는 대규모 캠페인을 통해 한국의 소프트웨어, IT, 금융, 반도체, 통신 산업을 동시 다발적으로 공격했다. 2024년 11월부터 2025년 2월 사이, 최소 6개 조직이 직접 피해를 입었으며, 공격에 악용된 소프트웨어의 보급률을 감안할 때 실제 피해 기업은 훨씬 더 많을 것으로 추정된다.

문제의 심각성은 피해 대상에서도 드러난다. 랜섬웨어 감염 피해의 94%가 중견·중소기업에서 발생했다. 대기업은 수억 원을 투자해 전문 보안팀을 운영하고 최신 방어 시스템을 갖출 수 있지만, 대부분의 중소기업은 그럴 여력이 없다. 그러나 공격자는 누구도 가리지 않는다.

### 1-2. 왜 기존 보안 장비로는 막을 수 없나?

많은 기업이 이미 백신, 방화벽, 침입 탐지 시스템을 갖추고 있다. 그런데도 왜 공격은 성공하는가?

현대의 사이버 공격은 단순한 바이러스가 아니다. 공격자는 정상적인 문서처럼 보이는 파일을 만들고, 보안 장비의 탐지를 우회하기 위해 암호를 설정하며, OLE 개체 기능 같은 정상 기능을 악용한다. 심지어 Github 같은 합법적인 클라우드 서비스를 악용하여 악성코드를 저장하고 관리하기도 한다. 이는 마치 정문이 아닌 배달원으로 위장해 뒷문으로 침입하는 것과 같다.

더 큰 문제는 공격 기법이 끊임없이 진화한다는 점이다. 보안 장비는 과거에 알려진 공격 패턴을 차단하도록 설정되어 있다. 하지만 공격자는 매번 새로운 방법을 시도한다. 결과적으로 보안팀은 보안 장비에 의존한다면 패치를 수행할 때까지 무방비 상태이며, 한 발 뒤처진 대응을 할 수 밖에 없는 악순환에 빠진다.

### 1-3. 해결책: 공격받기 전에 먼저 공격해보기

이 문제를 해결하기 위해 보안 업계는 새로운 접근법을 개발했다. "실제 공격자처럼 우리 시스템을 공격해보면 어떨까?"

이것이 바로 레드팀 작전(Red Team Operation)이다. 레드팀은 기업의 디지털 시스템, 직원, 애플리케이션을 실제 공격자가 사용하는 기법으로 시뮬레이션하여 테스트한다. 마치 소방훈련처럼, 진짜 불은 아니지만 실전처럼 대응 능력을 점검하는 것이다.

레드팀 작전의 결과는 방어팀(블루팀)이 특정 공격에 대한 탐지 규칙을 작성하고, 보안 통제를 구현하며, 인프라를 강화하고, 취약점을 패치하는 데 활용된다. 실제로 공격을 당하기 전에, 우리가 먼저 그 공격을 재현해보고 막을 수 있는지 확인하는 것이다.

### 1-4. Adversary Emulation: 특정 공격자를 정확히 재현하기

그런데 단순히 "뭔가 공격해보기"만으로는 부족하다. Lazarus는 금융기관 공격에 특화되어 있고, Kimsuky는 정부기관을 타깃으로 한다. 각 공격 그룹마다 선호하는 기법이 다르다.

적대자 에뮬레이션(Adversary Emulation)은 특정 공격자의 전술(Tactics), 기법(Techniques), 절차(Procedures), 즉 TTPs를 정확히 재현하는 과정이다. 예를 들어 Kimsuky가 법무법인을 공격할 때 사용했던 정확한 방법을 우리 환경에서도 똑같이 시도해보는 것이다.

이를 위해 보안 업계는 표준화된 언어를 만들었다. MITRE ATT&CK 프레임워크는 전 세계 공격 기술을 체계적으로 정리한 백과사전 같은 것이다. "T1003.001"이라고 하면 전 세계 보안 전문가가 "아, Mimikatz로 자격증명 탈취하는 기법"이라고 이해한다.

MITRE Caldera는 한 단계 더 나아간다. Caldera는 ATT&CK에 정의된 공격 기술을 안전한 환경에서 자동으로 시뮬레이션할 수 있는 프레임워크다. 즉, "T1003.001 기법을 우리 시스템에서 실제로 실행해보세요"라고 명령하면, 격리된 가상 환경에서 실제 피해 없이 그 공격을 재현하고 우리 방어 시스템이 이를 탐지하는지 확인할 수 있다.

**하지만 여기서 문제가 생긴다.** Caldera는 실행 엔진일 뿐이다. "어떤 공격을 어떤 순서로 실행할지"는 사람이 직접 시나리오를 작성해야 한다. Kimsuky의 CTI 보고서를 읽고, 거기서 사용된 기법들을 파악하고, 하나하나 Caldera 시나리오로 변환하는 작업 말이다. 이 과정은 전문 지식이 필요하고, 시간이 오래 걸린다.

### 1-5. 현실의 벽: 한 번에 성공하는 시나리오는 드물다

이 문제를 해결하기 위해 2024년 발표된 AURORA 시스템(Wang et al.)은 CTI 보고서를 읽어서 자동으로 Caldera 공격 시나리오를 생성한다. 사람이 수동으로 작성하면 180분 이상 걸리던 작업을 5분으로 단축했다.

하지만 **생성된 시나리오의 초기 실행 성공률은 77%에 불과했다.** 23%는 실패했다.

주요 실패 원인은 권한 부족, 도구 누락, 환경 차이다. 예를 들어 Metasploit exploit이 10개 이상의 필수 인자를 요구하면, LLM은 일부만 식별하고 나머지를 놓쳐서 "Missing required option" 에러가 발생한다.

**더 큰 문제는 AURORA가 시나리오를 만들고 나면 끝이라는 것이다.** 실행 중 에러가 발생하면? AURORA는 멈춘다. 사람이 다시 개입해서 무엇이 잘못되었는지 분석하고, 시나리오를 수정하고, 다시 실행해야 한다. 이 과정에 평균 30-60분이 걸린다. 자동화로 아낀 시간을 다시 수동 작업으로 쓰는 셈이다.

---

## 2. 연구 목표 및 접근 방법

### 2-1. 연구 목표

본 연구는 KISA TTPs 보고서를 Caldera 실행 시나리오로 자동 변환하는 시스템을 구축한다. 특히 실행 실패 시 자동으로 원인을 분석하고 수정하는 Self-Correcting 메커니즘을 구현하여, 실행 성공률을 개선하는 것을 목표로 한다.

### 2-2. 핵심 아이디어: Self-Correcting

1-5에서 확인했듯이, AURORA는 시나리오를 생성하고 1회 실행하면 끝이다. 실패 시 사람이 개입해야 하며, 이 과정에 평균 30-60분이 소요된다. 또한 실패한 시나리오를 수정 없이 반복 실행하는 단순 재시도 방식도 환경이 동일하면 같은 실패가 반복될 뿐이다.

본 연구는 다르게 접근한다. 실행 실패 시 원인을 자동으로 진단하고, 이전과 다른 방법으로 수정하여 재실행하는 피드백 루프를 구현한다. 예를 들어 권한 부족으로 실패하면 권한 상승을 시도하거나 대안 기법으로 전환한다.

**실행 → 실패 → 진단 → 수정 → 재실행**

Self-Correcting의 핵심 메커니즘은 네 가지다.

첫째, **누적된 컨텍스트 활용**이다. 모든 이전 시도의 YAML, 에러, 진단 결과를 LLM에게 제공하여 이미 실패한 방법을 피하도록 한다.

둘째, **환경 정보 기반 수정**이다. Caldera Agent API로 권한 레벨, 사용 가능한 executor, OS 버전을 조회하여 환경 제약에 맞는 수정안을 생성한다.

셋째, **능동적 진단**이다. 에러 메시지만 읽는 것이 아니라, 실패 원인을 파악하기 위한 추가 명령어를 자동 실행한다. 권한 오류 시 whoami /priv로 실제 권한을 확인하고, 도구 누락 시 where.exe로 경로를 탐색하는 식이다.

넷째, **명시적 제약 부여**다. LLM에게 "이미 시도한 방법을 반복하지 말라"는 명확한 지시를 전달한다.

보안 공격 시나리오는 환경 의존성이 크다. 같은 Mimikatz 명령도 권한, 경로, 보안 설정에 따라 다르게 작동한다. Self-Correcting은 이러한 환경 차이를 자동으로 파악하고 적응하는 것을 목표로 한다.

모든 실패를 복구할 수는 없다. 필수 도구가 완전히 없거나 취약점이 패치된 경우는 복구 불가능하다. 본 연구는 어떤 유형의 실패가 자동 복구 가능한지, Self-Correcting이 성공률을 얼마나 개선하는지를 검증하는 것이 목표이다.

### 2-3. 구현 방법

**전체 파이프라인 개요**

![image.png](attachment:e73e4890-36f9-4ab4-9e91-460cd2af4398:image.png)

4단계로 나눈 이유는 각 단계의 성능을 독립적으로 측정하기 위함이다. 예를 들어 최종 실행 성공률이 낮다면, M1의 TTP 추출 정확도 문제인지, M2의 YAML 생성 품질 문제인지, 아니면 M3의 환경 제약 때문인지 구분할 수 있다.

### M1. TTP 추출 시스템 구축

**입력:** KISA TTPs 보고서 11편 (PDF)

**출력:** 구조화된 TTPs (JSON: Technique ID, Tactic, Description)

**목적:** 한국어 비정형 텍스트에서 MITRE ATT&CK 매핑 가능한 정보 추출

**수행 방법:**

- 연구자 2명이 독립적으로 Ground Truth 구축
- GPT-4o, Claude, Gemini로 TTP 추출 수행
- Precision, Recall, accuracy로 평가

### M2. 실행 가능 시나리오 생성

**입력:** M1의 TTPs (JSON)

**출력:** Caldera YAML 파일

**목적:** ATT&CK 기법을 실제 실행 가능한 명령어로 변환

**수행 방법:**

- GPT-4o, Claude, Gemini로 YAML 생성
- yamllint로 문법 검증
- Caldera 서버 로드 테스트
- MITRE ATT&CK 문서와 논리적 일치도 비교

문법적으로 올바르고 Caldera에 로드 가능한 YAML 생성률을 1차 지표로 삼는다. 실행 가능성은 M3에서 별도로 평가한다.

### M3. 테스트 환경 기반 실행 검증

**입력:** M2의 YAML 파일

**실행 환경:** Windows 10 Pro + Caldera

**출력:** 초기 실행 성공률, 실패 케이스 목록 및 원인 분류

**목적:** Self-Correcting을 적용하기 전, 생성된 시나리오의 기본 성공률을 측정합니다.

M3는 Self-Correcting의 효과를 검증하기 위한 비교 기준점을 확보하는 단계입니다. M2에서 생성된 모든 YAML 시나리오를 Caldera에서 **수정 없이 1회씩만 실행**합니다. 실행이 실패하더라도 재시도하지 않고, 성공 여부와 실패 원인만 기록합니다.

**실행 방식:**

- 각 시나리오를 Caldera Operation API로 실행
- exit code로 성공(0) 또는 실패(non-zero) 판정
- 실패한 경우 stderr, stdout, 환경 정보를 수집
- 실패 원인을 분류 (권한 부족, 도구 누락, 환경 제약, 명령어 오류 등)

**평가 방법:**

- 전체 시나리오 중 1회 실행으로 성공한 비율을 초기 성공률로 기록
- Caldera Operation Results API에서 exit code, stdout, stderr를 수집하여 실행 결과 분석
- 실패 사례 분류(권한 부족, 환경 제약, 명령어 오류 등)

**이 단계의 성공률이 M4의 baseline이 된다.** AURORA의 77%를 참고하되, 테스트 환경과 데이터셋 차이로 인해 직접 비교는 어려울 수 있다.

### M4. Self-Correcting 메커니즘 구현

**입력:** M3에서 실패한 케이스들

**출력:** 수정 후 재실행 결과, 최종 성공률, 개선도 분석

**목적:** 사람 개입 없이 실패를 자동으로 복구하여 성공률을 개선합니다.

M4는 본 연구의 핵심 기여인 Self-Correcting 메커니즘을 적용하는 단계입니다. M3에서 1회 실행으로 실패한 시나리오들을 대상으로, 실패 원인을 자동으로 분석하고 수정한 뒤 재실행하는 피드백 루프를 구현합니다.

**구현 방식: Caldera API 기반 자동화 루프**

```
실행_횟수 = 1 (M3에서 이미 1회 실패했으므로)

while 실행_횟수 < 3:
    1. 이전 실패 정보 분석
       - exit_code, stderr, stdout 수집
       - 실패 유형 자동 분류 (권한/도구/환경/문법)
    
    2. 진단 명령어 자동 실행
       - 실패 유형에 따라 적절한 진단 명령어 선택
       - 예: 권한 오류 → whoami /priv, whoami /groups 실행
       - 예: 도구 누락 → where.exe <tool>, Get-Command 실행
       - 예: 환경 문제 → Get-ExecutionPolicy, Test-Connection 실행
       - 진단 결과를 구조화하여 저장
    
    3. Agent 환경 정보 조회 (Caldera Agent API)
       - OS 버전, 현재 권한 레벨, 사용 가능한 executor
    
    4. LLM에게 종합 정보 제공
       - 원본 YAML
       - 모든 실패 히스토리 (YAML, 에러, 진단 결과)
       - 환경 정보
       - 지시: "진단 결과를 바탕으로 이미 실패한 방법과 다른 접근을 제시하라"
    
    5. 수정된 YAML로 재실행
       - Caldera에 새 시나리오 등록
       - Operation API로 실행
       - 결과 수집
    
    if exit_code == 0:
        성공 → 종료
    else:
        실행_횟수++
        히스토리에 이번 시도 추가 (YAML + 에러 + 진단)
        다시 1번부터
```

```cpp
procedure SELF_CORRECTING_REFINEMENT(T, Y₀, A, MAX_ITER)
    H ← ∅                                          // 빈 히스토리로 초기화
    Y_current ← Y₀
    iter ← 1
    
    while iter ≤ MAX_ITER do
        // 1단계: 시나리오 실행 및 결과 수집
        (exit_code, stdout, stderr) ← EXECUTE(Y_current, A)
        
        if exit_code = 0 then
            return (Y_current, SUCCESS, H)          // 성공 시 즉시 반환
        end if
        
        // 2단계: 실패 원인 진단
        failure_type ← CLASSIFY_FAILURE(stderr)
        D ← EXECUTE_DIAGNOSTICS(failure_type, A)    // 진단 명령어 실행
        
        // 3단계: 환경 정보 조회
        env ← GET_ENVIRONMENT(A)                    // OS, 권한, executor
        
        // 4단계: 히스토리 업데이트
        H ← H ∪ {(Y_current, exit_code, stderr, D)}
        
        // 5단계: 개선된 시나리오 생성
        context ← BUILD_CONTEXT(T, Y₀, H, env)
        Y_refined ← LLM_REFINE(context)             // 이전 실패를 피하는 새 YAML
        
        Y_current ← Y_refined
        iter ← iter + 1
    end while
    
    return (Y_current, FAILED, H)                   // 최대 시도 초과 시 실패
end procedure

function CLASSIFY_FAILURE(stderr)
    // 에러 메시지 패턴 매칭으로 실패 유형 분류
    if MATCH(stderr, "권한|Access denied") then
        return PERMISSION_ERROR
    else if MATCH(stderr, "없음|not found") then
        return TOOL_MISSING
    else if MATCH(stderr, "정책|execution policy") then
        return ENVIRONMENT_CONSTRAINT
    else
        return SYNTAX_ERROR
    end if
end function

function EXECUTE_DIAGNOSTICS(failure_type, A)
    // 실패 유형별 진단 명령어 선택 및 실행
    commands ← SELECT_DIAGNOSTIC_COMMANDS(failure_type)
    results ← ∅
    
    for each cmd in commands do
        output ← RUN(cmd, A)
        results ← results ∪ {(cmd, output)}
    end for
    
    return results
end function

function BUILD_CONTEXT(T, Y₀, H, env)
    // LLM에 전달할 종합 컨텍스트 구성
    return {
        "원본_TTP": T,
        "초기_YAML": Y₀,
        "환경정보": env,
        "실패_히스토리": H,
        "제약사항": "히스토리의 방법 반복 금지, 환경 호환 executor 사용"
    }
end function
```

**LLM 프롬프트 구조**

LLM에게 전달되는 정보는 다음과 같이 구조화된다.

```cpp
[원본 정보]
- 원본 TTP 설명 (M1에서 추출한 것)
- 최초 생성된 YAML (M2에서 생성한 것)

[환경 정보]
- OS: Windows 10 Pro (Build XXXXX)
- 현재 권한: User / Admin
- 사용 가능한 Executor: PowerShell, CMD, Python 등
- 설치된 도구: 확인된 경로 목록

[실패 히스토리]
시도 1:
  YAML: [첫 번째 시도의 YAML]
  exit_code: 1
  stderr: "ERROR kuhl_m_privilege_simple"
  
  진단 결과:
    whoami /priv:
      SeDebugPrivilege: Disabled
      SeBackupPrivilege: Not Present
    whoami /groups:
      BUILTIN\Users
      (Administrators 그룹 없음)
    Elevated: False
  
  분석: SeDebugPrivilege가 비활성화되어 있고 관리자 권한 없음

[지시사항]
위 실패 히스토리와 진단 결과를 분석하여, 이미 시도한 방법과 다른 접근으로 YAML을 수정하라.
- 시도 1에서 사용한 방법을 반복하지 말 것
- 시도 2에서 사용한 방법을 반복하지 말 것
- 진단 결과로 확인된 환경 제약(권한, 도구)을 고려할 것
- 동일한 ATT&CK 기법을 달성할 수 있는 대안을 제시할 것
```

이렇게 구조화된 프롬프트는 LLM이 단순히 "다시 만들어보라"는 요청이 아니라, "왜 실패했는지 이해하고, 그것을 피하는 새로운 방법을 찾아라"는 요청임을 명확히 한다. 특히 진단 결과를 포함함으로써, LLM이 추측이 아닌 사실에 기반하여 수정안을 제시할 수 있다.

**평가 방법:**

- 초기 성공률 (M3 baseline) vs 최종 성공률 (M4 after correction) 비교
- 1회 수정으로 성공한 케이스, 2회 수정으로 성공한 케이스 각각 집계
- 실패 원인 유형별 수정 성공률 분석 (어떤 유형의 오류가 자동 복구 가능한가?)
- 수정 불가능한 케이스 분석 (근본적 환경 제약, 필수 도구 부재 등)

예를 들어 M3에서 7개가 실패했는데, M4에서 Self-Correcting을 통해 4개를 추가로 성공시켰다면, 최종 성공률은 (23+4)/30 = 90%가 되어 13%p 개선된 것입니다. 이 개선 정도가 Self-Correcting의 효과이다.

### 2-4. 실행 계획

**전체 기간: 8주 (팀원 2명)**

| 주차 | 목표 | 주요 활동 | 산출물 |
| --- | --- | --- | --- |
| 1주 | 환경 구축 | Windows 10 + Caldera 설치, KISA 보고서 11편 수집 | 테스트 환경, 데이터셋 |
| 2-3주 | M1 | TTP 추출 시스템 개발, Ground Truth 구축, 모델 비교 평가 | TTP 추출 시스템, 평가 보고서 |
| 4주 | M2 | YAML 생성 시스템 개발, 문법 검증 및 Caldera 로드 테스트 | YAML 생성 시스템, 검증 결과 |
| 5주 | M3 | 시나리오 실제 실행, 로그 수집 및 실패 케이스 분석 | Baseline 성공률, 실패 원인 분류 |
| 6-7주 | M4 | Self-Correcting 루프 구현, Caldera API 연동 | Self-Correcting 시스템 |
| 8주 | 통합 평가 | End-to-End 테스트, 성능 측정 및 분석, 최종 보고서 작성 | 최종 시스템, 성능 비교 보고서 |

**역할 분담**

| 구분 | 연구자 1 | 연구자 2 |
| --- | --- | --- |
| **주 담당** | M1 (TTP 추출), M3 (실행 검증) | M2 (YAML 생성), M4 (Self-Correcting) |
| **세부 업무** | Caldera 환경 설정, Ground Truth 구축, TTP 추출 프롬프트 개발, 실행 로그 분석 | KISA 데이터 수집, YAML 생성 프롬프트 개발, Self-Correcting 루프 구현, 최종 보고서 작성 |
| **공동 작업** | 모델 성능 평가 (3, 5주), End-to-End 통합 테스트 (8주), 결과 분석 및 논의 (전 기간) |  |

---

## 3. 기대효과

### 3-1. 기존 연구 대비 차별점

**1. Self-Correcting 메커니즘 도입**

본 연구는 실행 실패를 자동으로 복구하는 메커니즘을 시도한다.

- **기존 접근**: 시나리오 생성 → 실행 → (실패 시 종료)
- **본 연구**: 시나리오 생성 → 실행 → 실패 감지 → 자동 분석 → 수정 → 재실행

Caldera API로 오류 정보를 자동 수집하고, 이전 실패 히스토리를 LLM에게 제공하여 점진적 개선을 시도한다. 사람 개입 없이 최대 3회 자동 재시도한다.

이 접근법이 실제로 성공률을 개선하는지, 어떤 유형의 실패가 복구 가능한지 검증하고자 한다. AURORA의 77%를 참고하되, 환경과 데이터셋 차이로 직접 비교는 어려울 수 있다.

**2. 한국어 CTI 환경에서의 적용 가능성 확인**

기존 연구(AURORA, TRAM 등)는 모두 영어만 대상으로 했다. 본 연구는 KISA 보고서로 유사한 결과를 낼 수 있는지 탐색한다.

영어 대비 한국어의 성능 격차를 분석하여, 언어적 특수성이 자동화에 미치는 영향을 파악하고자 한다.

### 3-2. 학술적 기여

**Self-Correcting 접근법의 효과 측정**

기존 연구는 "한 번에 완벽한 시나리오"를 만들려 했다. 본 연구는 "실패 → 수정 → 재실행" 피드백 루프가 성공률 개선에 기여하는지 측정한다. Context 누적 방식의 효과와 한계를 분석하여, 향후 유사 연구의 참고 자료를 제공할 수 있을 것으로 기대된다.

**다국어 CTI 자동화 연구를 위한 기초 자료**

영어 기반 방법론이 다른 언어 환경에서도 작동하는지 확인한다. 한국어 처리의 한계와 가능성을 분석하여, 다국어 연구의 출발점을 제공하고자 한다.

### 3-3. 실무적 활용 가능성

**자동 복구를 통한 안정성 향상 가능성**

Self-Correcting 메커니즘이 의도대로 작동한다면, 다양한 환경에서 안정적으로 작동할 가능성이 있다. 한 번에 성공하지 않아도 자동 복구를 시도하므로, 실무 적용 장벽을 낮출 수 있을 것으로 보인다.

**국내 보안팀의 위협 검증 자동화 기반**

KISA 보고서를 자동 처리할 수 있다면, 국내 보안팀이 최신 위협을 검증할 기반이 마련될 수 있다. 특히 전문 인력이 부족한 중소기업이 위협을 자체 평가하는 데 참고할 수 있을 것으로 기대된다.

### 3-4. 연구의 한계

본 연구는 탐색적 성격이 강하며, 다음 한계가 있다:

**환경의 제약**

- Windows 10 기본 환경에서 재현 가능한 공격만 다룸
- 특수 인프라가 필요한 공격 제외

**데이터셋 규모**

- KISA 보고서 11편으로 제한
- 통계적 유의성 확보 어려움

**Self-Correcting의 불확실성**

- 환경과 공격 유형에 따라 개선 효과가 다를 수 있음
- 근본적 환경 제약(필수 도구 부재, 패치된 취약점)은 복구 불가능
- LLM의 추론 능력 한계로 인한 수정 실패 가능

**향후 과제**

- 더 다양한 환경과 대규모 데이터셋으로 검증 필요
- 실제 기업 환경에서의 적용 사례 연구 필요
- Self-Correcting의 장기적 학습 효과 탐색 필요